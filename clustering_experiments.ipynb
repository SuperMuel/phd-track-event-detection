{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 84593 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_FILE=\"newspapers_filtered_2024-04-30_18-17-52.jsonl\"\n",
    "\n",
    "from json import loads\n",
    "\n",
    "data = [\n",
    "    loads(line)\n",
    "    for line in open(DATA_FILE, \"r\", encoding=\"utf-8\").readlines()\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(data)} records\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 84593 embeddings\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_FILE = f\"{DATA_FILE}_embeddings.npy\"\n",
    "\n",
    "embeddings = np.load(EMBEDDINGS_FILE)\n",
    "\n",
    "print(f\"Loaded {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add embeddings to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embedding\"] = [e for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             International de Sète : la pétanque, une affai...\n",
       "text              L'international de pétanque de Sète, avait lie...\n",
       "date                                            2022-03-07 00:00:00\n",
       "article_id                                                  2047761\n",
       "article_url       https://france3-regions.francetvinfo.fr/occitanie\n",
       "article_domain                      france3-regions.francetvinfo.fr\n",
       "embedding         [-0.02630615234375, 0.0106658935546875, 0.0487...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=50, max_iter=1000)\n",
    "model.fit(df['embedding'].to_list())\n",
    "df[\"cluster\"] = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show one random cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*df[df.cluster==23].title.head(100), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the subjects of each cluster. One can be about violence and crime, another about politics, another about sports, etc.\n",
    "\n",
    "We found a cluster about rugby only, and what's amazing with this embedding approach, is that we can find related subjects even if they don't use the same words. \n",
    "Example : `Pro D2. FCG - Nevers : « On a toutes les cartes pour y répondre comme il faut », l'Ardéchois Luka Plataret veut réussir un coup au Stade des Alpes` doesn't contain the word `rugby` but is still in the rugby cluster.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping by week\n",
    "\n",
    "The goal of this section if to group articles by week and find the most important topics of each week. \n",
    "\n",
    "For this, we will use the following approach :\n",
    "- For each week, we will find the most important clusters\n",
    "- We will then try to find the subject of each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a year-week column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"year_week\"] = df[\"date\"].dt.strftime(\"%Y-%U\")\n",
    "\n",
    "assert df[\"year_week\"].value_counts().sum() == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of articles per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "sns.countplot(data=df, x=\"year_week\", order=df[\"year_week\"].value_counts().index)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Number of articles per week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df.groupby(\"year_week\")\n",
    "\n",
    "for week, group in grouped_data:\n",
    "    print(f\"Processing week {week}\")\n",
    "\n",
    "    N_CLUSTERS = int(len(group) / 10)\n",
    "\n",
    "    model = KMeans(n_clusters=N_CLUSTERS, max_iter=1000)\n",
    "    model.fit(group['embedding'].to_list())\n",
    "    df.loc[df.year_week == week, \"cluster\"] = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = df[df['year_week'] == '2022-11' ]\n",
    "\n",
    "# find the biggest cluster\n",
    "\n",
    "biggest_cluster = week['cluster'].value_counts().idxmax()\n",
    "\n",
    "week[week['cluster'] == biggest_cluster].title.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for week, group in grouped_data:\n",
    "    print(f\"Processing week {week}\")\n",
    "    # print 2 biggest clusters\n",
    "    for cluster in group['cluster'].value_counts().index[:2]:\n",
    "        print(f\"Cluster {cluster}\")\n",
    "        print(*group[group.cluster==cluster].title.head(100), sep='\\n\\n')\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest clusters created by KMeans do not always to represent the most important topics of the week.\n",
    "We need an algorithms that is indifferent to the size of the clusters.\n",
    "\n",
    "We should use DBSCAN or hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "grouped_data = df.groupby(\"year_week\")\n",
    "\n",
    "N_CLUSTERS = 50\n",
    "#LINKAGE = 'ward' #  or 'single', 'complete', or 'average' \n",
    "LINKAGE = 'single'\n",
    "#LINKAGE = 'complete'\n",
    "#LINKAGE = 'average'\n",
    "\n",
    "\n",
    "for week, group in grouped_data:\n",
    "    print(f\"Processing week {week}\")\n",
    "    model = AgglomerativeClustering(n_clusters=N_CLUSTERS, linkage='ward')\n",
    "for week, group in grouped_data:\n",
    "    print(f\"Processing week {week}\")\n",
    "    labels = model.fit_predict(group['embedding'].to_list())\n",
    "    df.loc[df.year_week == week, \"cluster\"] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze the clusters for each week\n",
    "for week, group in grouped_data:\n",
    "    print(f\"Processing week {week}\")\n",
    "\n",
    "    # find 4 biggest clusters\n",
    "    biggest_clusters = group['cluster'].value_counts().index[:4]\n",
    "\n",
    "    for cluster in biggest_clusters:\n",
    "        print(f\"Cluster {cluster}\")\n",
    "        print(*group[group.cluster==cluster].title.head(100), sep='\\n\\n')\n",
    "        print(\"\\n\\n --- \\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DBSCAN\n",
    "\n",
    "Since \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality using PCA\n",
    "N_COMPONENTS = 25  # Choose the number of components based on your data\n",
    "\n",
    "pca = PCA(n_components=N_COMPONENTS)\n",
    "reduced_embeddings = pca.fit_transform(df['embedding'].tolist())\n",
    "\n",
    "# Update the DataFrame with the reduced embeddings\n",
    "df['reduced_embedding'] = reduced_embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "\n",
    "MIN_SAMPLES = 10\n",
    "MAX_EPS = 10\n",
    "\n",
    "for week, group in grouped_data:\n",
    "    print(f\"Processing week {week}\")\n",
    "    model = OPTICS(min_samples=MIN_SAMPLES, max_eps=MAX_EPS, metric='cosine')\n",
    "    labels = model.fit_predict(group['reduced_embedding'].tolist())\n",
    "    df.loc[df.year_week == week, \"cluster\"] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of clusters for each week\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.countplot(data=df, x=\"year_week\", hue=\"cluster\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Number of articles per week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the clusters for each week\n",
    "for week, group in grouped_data:\n",
    "    print(f\"Processing week {week}\")\n",
    "\n",
    "    # Find the unique clusters (excluding noise points)\n",
    "    clusters = group['cluster'].unique()\n",
    "    clusters = clusters[clusters != -1]  # Exclude noise points\n",
    "\n",
    "    # Sort clusters by size (descending order)\n",
    "    cluster_sizes = group['cluster'].value_counts()\n",
    "    sorted_clusters = cluster_sizes.loc[clusters].index\n",
    "\n",
    "    # Print the titles for each cluster\n",
    "    for cluster in sorted_clusters[:2]:\n",
    "        print(f\"Cluster {cluster}\")\n",
    "        titles = group[group.cluster == cluster].title.tolist()\n",
    "        print(*titles[:100], sep='\\n\\n')\n",
    "        print(\"\\n\\n --- \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN\n",
    "\n",
    "In this section and as in the [Using LLM for Improving Key Event Discovery:\n",
    "Temporal-Guided News Stream Clustering with Event Summaries](https://openreview.net/pdf?id=lojtRAQOls) paper, we will use HDBSCAN to find the most important topics of each week. \n",
    "\n",
    "This algorithm is very useful because it is indifferent to the size, shape and density of the clusters, while requiring only one parameter, the MIN_CLUSTER_SIZE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLUSTER_SIZE = 3\n",
    "\n",
    "\n",
    "for week, group in grouped_data:\n",
    "    # get the progression (e.g  \"34/123\")\n",
    "    \n",
    "    print(f\"Processing week {week}\")\n",
    "    model = hdbscan.HDBSCAN(min_cluster_size=MIN_CLUSTER_SIZE, metric='euclidean', cluster_selection_method='eom')\n",
    "    labels = model.fit_predict(group['embedding'].tolist())\n",
    "    df.loc[df.year_week == week, \"cluster\"] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the week as index, and the number of clusters for that week\n",
    "df_clusters = df.groupby(\"year_week\")[\"cluster\"].nunique().reset_index()\n",
    "\n",
    "# Plot the number of clusters for each week\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.barplot(data=df_clusters, x=\"year_week\", y=\"cluster\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Number of clusters per week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the clusters for each week\n",
    "for week, group in grouped_data:\n",
    "    print(f\"Processing week {week}\")\n",
    "\n",
    "    # Find the unique clusters (excluding noise points)\n",
    "    clusters = group['cluster'].unique()\n",
    "    clusters = clusters[clusters != -1]  # Exclude noise points\n",
    "\n",
    "    # Sort clusters by size (descending order)\n",
    "    cluster_sizes = group['cluster'].value_counts()\n",
    "    sorted_clusters = cluster_sizes.loc[clusters].index\n",
    "\n",
    "    # Print the titles for each cluster\n",
    "    for cluster in sorted_clusters[:2]:\n",
    "        print(f\"Cluster {cluster}\")\n",
    "        titles = group[group.cluster == cluster].title.tolist()\n",
    "        print(*titles[:100], sep='\\n\\n')\n",
    "        print(\"\\n\\n --- \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the datea as a clustering feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to a numerical feature\n",
    "df['date_feature'] = df['date'].astype('int64') / 10**18  # Adjust the scaling factor as needed\n",
    "\n",
    "# Combine the date feature with the embeddings\n",
    "df['combined_features'] = df.apply(lambda row: np.append(row['embedding'], row['date_feature']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLUSTER_SIZE = 40\n",
    "\n",
    "# take only first 2 months\n",
    "df_restricted = \n",
    "\n",
    "model = hdbscan.HDBSCAN(min_cluster_size=MIN_CLUSTER_SIZE, metric='euclidean', cluster_selection_method='eom')\n",
    "labels = model.fit_predict(df['combined_features'].tolist())\n",
    "df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of clusters for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the date as index, and the number of clusters for that date\n",
    "df_clusters = df.groupby(df['date'].dt.date)['cluster'].nunique().reset_index()\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.barplot(data=df_clusters, x='date', y='cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Number of clusters per date\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
